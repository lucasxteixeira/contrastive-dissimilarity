{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import scipy.io\n",
    "import scipy.io.arff\n",
    "\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import sklearn.cluster\n",
    "import sklearn.discriminant_analysis\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "\n",
    "import imblearn\n",
    "import imblearn.over_sampling\n",
    "\n",
    "import torch\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a numpy array\n",
    "X = ...\n",
    "Y = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix and target vector\n",
    "X = data.drop(outcome, axis = 1).values\n",
    "Y = data[outcome].values\n",
    "\n",
    "# Encode the categorical target labels to integers\n",
    "encoder = sklearn.preprocessing.LabelEncoder()\n",
    "Y = encoder.fit_transform(Y)\n",
    "\n",
    "# Get unique classes and their counts\n",
    "classes, counts = np.unique(Y, return_counts=True)\n",
    "\n",
    "# Display the number of unique classes\n",
    "print(f\"Classes: {len(classes)}\")\n",
    "\n",
    "# Calculate the imbalance ratio (IR)\n",
    "majc = np.max(counts)\n",
    "minc = np.min(counts)\n",
    "ir = majc / minc\n",
    "print(f\"IR: {ir:.2f}\")\n",
    "\n",
    "# Create a data frame to store the f1-scores for each evaluated method\n",
    "f1_scores = pd.DataFrame(columns = [f\"Fold_{i}\" for i in range(1, 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for generating batches of pairs\n",
    "def get_pair_batch(batch_size, X, Y):\n",
    "\n",
    "  # Get the number of unique classes in Y\n",
    "  n_classes = max(Y) + 1\n",
    "\n",
    "  # Randomly select 'batch_size' number of classes\n",
    "  classes = np.random.choice(np.arange(n_classes), size = batch_size)\n",
    "\n",
    "  # Get the number of features in X\n",
    "  n_features = X.shape[1]\n",
    "\n",
    "  # Initialize arrays to store the pairs and their classes\n",
    "  pairs = [np.zeros((batch_size, n_features), dtype = np.float32) for i in range(3)]\n",
    "\n",
    "  # Store the classes for later filtering of positive and negative samples\n",
    "  pairs[2] = classes\n",
    "\n",
    "  for i in range(batch_size):\n",
    "\n",
    "    # Get indices of all samples that belong to the chosen class\n",
    "    choices = np.where(Y == classes[i])[0]\n",
    "\n",
    "    # Randomly select two samples of the same class\n",
    "    idx_A = np.random.choice(choices)\n",
    "    idx_B = np.random.choice(choices)\n",
    "\n",
    "     # Save the samples to the pair list\n",
    "    pairs[0][i] = X[idx_A]\n",
    "    pairs[1][i] = X[idx_B]\n",
    "\n",
    "  return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch data utils\n",
    "def batch_to_torch(pair_batch, device = None):\n",
    "\n",
    "  # Unpack the pair batch into individual variables\n",
    "  x1, x2, y = pair_batch\n",
    "\n",
    "  if device is not None:\n",
    "    # Convert arrays to PyTorch tensors and move to the specified device\n",
    "    x1 = torch.tensor(x1).to(device)\n",
    "    x2 = torch.tensor(x2).to(device)\n",
    "    y = torch.tensor(y).to(device)\n",
    "\n",
    "  return (x1, x2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NT-Xent loss function\n",
    "class NTXentLoss(torch.nn.Module):\n",
    "  def __init__(self, temperature = 0.5):\n",
    "    super(NTXentLoss, self).__init__()\n",
    "    self.temperature = temperature\n",
    "\n",
    "  def forward(self, diss, y):\n",
    "    size = diss.shape[0]\n",
    "\n",
    "    # Mask for positive samples\n",
    "    y = torch.cat([y, y], dim = 0)\n",
    "    y1 = torch.tile(y, [size])\n",
    "    y2 = torch.repeat_interleave(y, size, axis = 0)\n",
    "    pos_mask = torch.reshape(y1 == y2, (size, size))\n",
    "    pos_mask.diagonal().fill_(False)\n",
    "\n",
    "    # Mask for negative samples\n",
    "    neg_mask = (~torch.eye(size, size, device = diss.device, dtype = bool)).float()\n",
    "\n",
    "    # Compute nominator\n",
    "    nominator = torch.sum(pos_mask * torch.exp(diss / self.temperature), dim = 1)\n",
    "\n",
    "    # Compute denominator\n",
    "    denominator = torch.sum(neg_mask * torch.exp(diss / self.temperature), dim = 1)\n",
    "\n",
    "    # Compute loss\n",
    "    loss_partial = -torch.log(nominator / denominator)\n",
    "    loss = torch.mean(loss_partial)\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Projection head\n",
    "class ProjectionHead(torch.nn.Module):\n",
    "  def __init__(self, inpt, layers, dropout_rate = 0.3):\n",
    "    super(ProjectionHead, self).__init__()\n",
    "\n",
    "    # Initialize layers\n",
    "    hidden_layers = [\n",
    "      torch.nn.Linear(inpt, layers[0]),\n",
    "      torch.nn.ReLU(),\n",
    "      torch.nn.Dropout(dropout_rate)\n",
    "    ]\n",
    "\n",
    "    # Add additional hidden layers\n",
    "    for i in range(len(layers) - 1):\n",
    "      hidden_layers.extend([\n",
    "        torch.nn.Linear(layers[i], layers[i + 1]),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Dropout(dropout_rate)\n",
    "      ])\n",
    "\n",
    "    # Add output layer\n",
    "    hidden_layers.append(torch.nn.Linear(layers[-1], 1))\n",
    "\n",
    "    # Convert into a simple sequential model\n",
    "    self.projection_head = torch.nn.Sequential(*hidden_layers)\n",
    "\n",
    "  def forward(self, x1, x2):\n",
    "    return self.projection_head(x1 - x2)\n",
    "\n",
    "# Contrastive model\n",
    "class ContrastiveModel(torch.nn.Module):\n",
    "  def __init__(self, inpt, layers = [128, 64, 32]):\n",
    "    super(ContrastiveModel, self).__init__()\n",
    "    self.projection_head = ProjectionHead(inpt, layers)\n",
    "\n",
    "  def forward(self, x1, x2):\n",
    "    if self.training:\n",
    "      batch_size = x1.shape[0]\n",
    "\n",
    "      x = torch.cat([x1, x2])\n",
    "\n",
    "      # Repeat the elements to match the input expected the network\n",
    "      x1 = torch.tile(x, [batch_size * 2, 1])\n",
    "      x2 = torch.repeat_interleave(x, batch_size * 2, axis = 0)\n",
    "\n",
    "      # Forward\n",
    "      dissimilarity = self.projection_head(x1, x2)\n",
    "      dissimilarity = torch.reshape(dissimilarity, (batch_size * 2, -1))\n",
    "\n",
    "    else:\n",
    "      dissimilarity = self.projection_head(x1, x2)\n",
    "\n",
    "    return (dissimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, Y_train, model_id, temperature = 0.5, batch_size = 32,\n",
    "                learning_rate = 1e-3, layers = [128, 64, 32], iterations = 10000):\n",
    "\n",
    "  # Define the model filename based on model_id\n",
    "  model_filename = f\"models/model_{model_id}.pth\"\n",
    "  print(model_filename)\n",
    "\n",
    "  # Define computation device\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  # Initialize model and optimizer\n",
    "  contr_model = None\n",
    "  inpt_features = X_train.shape[1] # Number of input features\n",
    "\n",
    "  # Load pre-trained model if exists\n",
    "  if os.path.isfile(model_filename):\n",
    "    contr_model = ContrastiveModel(inpt_features, layers = layers)\n",
    "    contr_model.load_state_dict(torch.load(model_filename))\n",
    "    contr_model.to(device)\n",
    "\n",
    "  # Initialize loss function\n",
    "  contr_loss = NTXentLoss(temperature)\n",
    "\n",
    "  # Train a new model if not loaded\n",
    "  if contr_model is None:\n",
    "    contr_model = ContrastiveModel(inpt_features, layers = layers)\n",
    "    contr_model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(contr_model.parameters(), lr = learning_rate)\n",
    "    contr_model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    for i in range(iterations):\n",
    "\n",
    "      # Generate a batch of pairs and move to device\n",
    "      x1, x2, y = batch_to_torch(get_pair_batch(batch_size, X_train, Y_train), device)\n",
    "\n",
    "      # Zero the gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      diss = contr_model(x1, x2)\n",
    "\n",
    "      # Compute loss and perform a backward pass\n",
    "      loss = contr_loss(diss, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # Update and display the training loss\n",
    "      train_loss += loss.item()\n",
    "      try:\n",
    "        average_train_loss = train_loss / ((i + 1) % 1000)\n",
    "        print(f\"\\r{i}: Train loss: {average_train_loss:.4f}\", end = \"\\r\")\n",
    "      except ZeroDivisionError:\n",
    "        print(f\"{i + 1}: Train loss: {train_loss / 1000:.4f}\")\n",
    "        train_loss = 0\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(contr_model.state_dict(), model_filename)\n",
    "\n",
    "  # Freeze the model parameters\n",
    "  for param in contr_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "  # Set to evaluation mode\n",
    "  contr_model.eval()\n",
    "\n",
    "  return contr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing\n",
    "# The testing set will be the same for all scenarios: methods and folds\n",
    "X_train_full, X_test_full, Y_train_full, Y_test = sklearn.model_selection.train_test_split(X, Y,\n",
    "                                                                                      test_size = 0.3,\n",
    "                                                                                      random_state = seed,\n",
    "                                                                                      stratify = Y)\n",
    "\n",
    "# Initialize dictionaries to store train datasets\n",
    "X_train, Y_train, X_test = {}, {}, {}\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Calculate total number of samples in the dataset\n",
    "total_samples = X_train_full.shape[0]\n",
    "\n",
    "# Generate train/test splits for different sample sizes\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Calculate number of samples for the current iteration\n",
    "  n_samples = math.ceil(i / 10 * total_samples)\n",
    "\n",
    "  # Perform stratified splitting of the data\n",
    "  if i < 10:\n",
    "    X_train[i], _, Y_train[i], _ = sklearn.model_selection.train_test_split(X_train_full, Y_train_full,\n",
    "                                                                            train_size = n_samples,\n",
    "                                                                            random_state = seed,\n",
    "                                                                            stratify = Y_train_full)\n",
    "  else:\n",
    "    X_train[i], Y_train[i] = X_train_full, Y_train_full\n",
    "\n",
    "  # Standardize the features\n",
    "  scaler = sklearn.preprocessing.StandardScaler()\n",
    "  scaler.fit(X_train[i])\n",
    "  X_train[i] = scaler.transform(X_train[i])\n",
    "  X_test[i] = scaler.transform(X_test_full)\n",
    "\n",
    "# Initialize dictionaries for the augmented dataset\n",
    "X_train_smote, Y_train_smote = {}, {}\n",
    "\n",
    "# Perform SMOTE oversampling for each train dataset\n",
    "for i in range(1, 11):\n",
    "  sampl = imblearn.over_sampling.SMOTE(random_state = seed, k_neighbors = 5)\n",
    "  X_train_smote[i], Y_train_smote[i] = sampl.fit_resample(X_train[i], Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the trained models\n",
    "models = {}\n",
    "\n",
    "# Loop through the different train datasets to train models\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Generate a unique model identifier based on the current iteration\n",
    "  model_id = f\"k-{i}\"\n",
    "\n",
    "  # Train the model using the augmented dataset and specified hyperparameters\n",
    "  models[i] = train_model(\n",
    "    X_train_smote[i], Y_train_smote[i],\n",
    "    model_id = model_id,\n",
    "    temperature = 0.5,\n",
    "    layers = [512, 256, 128],\n",
    "    iterations = 2000,\n",
    "    batch_size = 128,\n",
    "    learning_rate = 1e-4\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prototype selection\n",
    "\n",
    "# Initialize variables\n",
    "n_prototypes = 5\n",
    "prototypes_filename = \"cache/prototypes.pkl\"\n",
    "\n",
    "# Load prototypes if they already exist\n",
    "if os.path.isfile(prototypes_filename):\n",
    "  with open(prototypes_filename, \"rb\") as f:\n",
    "    prototypes = pickle.load(f)\n",
    "\n",
    "else:\n",
    "  # Initialize dictionary to store prototypes\n",
    "  prototypes = {}\n",
    "\n",
    "  # Loop through different train datasets to compute prototypes\n",
    "  for i in range(1, 11):\n",
    "    print(f\"K-fold: {i}\")\n",
    "\n",
    "    # Initialize the prototypes dictionary for this fold\n",
    "    n_classes = len(np.unique(Y_train_smote[i]))\n",
    "    n_features = X_train_smote[i].shape[1]\n",
    "    total_prototypes = n_classes * n_prototypes\n",
    "\n",
    "    # Initialize K-means prototypes\n",
    "    prototypes[i] = np.zeros((total_prototypes, n_features), dtype = np.float32)\n",
    "\n",
    "    # Compute prototypes for each class\n",
    "    for c in range(n_classes):\n",
    "\n",
    "      # Extract features corresponding to the current class\n",
    "      X_embedding = X_train_smote[i][Y_train_smote[i] == c]\n",
    "      start, end = c * n_prototypes, (c + 1) * n_prototypes\n",
    "\n",
    "      # Compute K-means prototypes\n",
    "      kmeans = sklearn.cluster.KMeans(n_clusters = n_prototypes, init = \"k-means++\", random_state = 1234, n_init = \"auto\").fit(X_embedding)\n",
    "      centroids = kmeans.cluster_centers_\n",
    "      prototypes[i][start:end, :] = centroids\n",
    "\n",
    "  # Save the computed prototypes\n",
    "  with open(prototypes_filename, \"wb\") as f:\n",
    "    pickle.dump(prototypes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissimilarity space\n",
    "\n",
    "# Initialize variables\n",
    "diss_space_filename = \"cache/dissimilarity_space.pkl\"\n",
    "\n",
    "# Load precomputed dissimilarity space if exists\n",
    "if os.path.isfile(diss_space_filename):\n",
    "  with open(diss_space_filename, \"rb\") as f:\n",
    "    diss_space = pickle.load(f)\n",
    "\n",
    "else:\n",
    "  # Initialize dictionary to store the dissimilarity space matrix\n",
    "  diss_space = {}\n",
    "\n",
    "  # Loop through different train datasets to compute the dissimilarity space matrix\n",
    "  for i in range(1, 11):\n",
    "\n",
    "    # Initialize dictionarie to store dissimilarity measures\n",
    "    diss_space[i] = {}\n",
    "\n",
    "    # Expand dimensions for broadcasting in numpy operations\n",
    "    enc_train = X_train[i][:, np.newaxis, :]\n",
    "    enc_test = X_test[i][:, np.newaxis, :]\n",
    "\n",
    "    diss_space[i][\"train\"] = np.linalg.norm(enc_train - prototypes[i], axis = 2)\n",
    "    diss_space[i][\"test\"] = np.linalg.norm(enc_test - prototypes[i], axis = 2)\n",
    "\n",
    "  # Save the generated dissimilarity space\n",
    "  with open(diss_space_filename, \"wb\") as f:\n",
    "    pickle.dump(diss_space, f)\n",
    "\n",
    "# Classify using the dissimilarity space\n",
    "np.random.seed(seed)\n",
    "\n",
    "svm_parameters = [\n",
    "  {'kernel': ['rbf'],\n",
    "   'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "   'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}]\n",
    "\n",
    "# Loop through different train datasets\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Initialize best F1 score\n",
    "  best_f1 = 0\n",
    "\n",
    "  # List of classifiers to be used\n",
    "  classifiers = [\n",
    "    sklearn.model_selection.GridSearchCV(sklearn.svm.SVC(), svm_parameters, cv = 5, scoring = 'f1_macro', verbose = 0),\n",
    "    sklearn.ensemble.RandomForestClassifier(),\n",
    "    sklearn.naive_bayes.GaussianNB()\n",
    "  ]\n",
    "\n",
    "  for clf in classifiers:\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(diss_space[i][\"train\"], Y_train[i])\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = clf.predict(diss_space[i][\"test\"])\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "\n",
    "    # Update best F1 score\n",
    "    best_f1 = max(best_f1, f1)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"Space\", f\"Fold_{i}\"] = best_f1\n",
    "\n",
    "  print(f\"{i} -> {best_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive dissimilarity space\n",
    "\n",
    "# Initialize variables\n",
    "contr_diss_space_filename = \"cache/contr_dissimilarity_space.pkl\"\n",
    "\n",
    "# Load precomputed contrastive dissimilarity space if exists\n",
    "if os.path.isfile(contr_diss_space_filename):\n",
    "  with open(contr_diss_space_filename, \"rb\") as f:\n",
    "    contr_diss_space = pickle.load(f)\n",
    "\n",
    "else:\n",
    "  # Initialize dictionary to store the contr. dissimilarity space matrix\n",
    "  contr_diss_space = {}\n",
    "\n",
    "  # Loop through each fold\n",
    "  for i in range(1, 11):\n",
    "    contr_diss_space[i] = {}\n",
    "\n",
    "    for data_type, data_set in zip([\"train\", \"test\"], [X_train, X_test]):\n",
    "      # Convert data to float32 and fetch the prototypes\n",
    "      x = np.float32(data_set[i])\n",
    "      y = prototypes[i]\n",
    "\n",
    "      # Get dimensions for reshaping\n",
    "      n_enc = x.shape[0]\n",
    "      n_prot = y.shape[0]\n",
    "\n",
    "      # Reshape data\n",
    "      x = np.repeat(x, n_prot, axis = 0)\n",
    "      y = np.tile(y, [n_enc, 1])\n",
    "\n",
    "      # Convert to PyTorch tensors and move to GPU\n",
    "      x = torch.from_numpy(x).to(\"cuda\")\n",
    "      y = torch.from_numpy(y).to(\"cuda\")\n",
    "\n",
    "      # Compute dissimilarities\n",
    "      diss = models[i](x, y)\n",
    "      diss = np.array(np.split(np.squeeze(diss.cpu().numpy()), n_enc))\n",
    "\n",
    "      # Store dissimilarities\n",
    "      contr_diss_space[i][data_type] = diss\n",
    "\n",
    "  # Save the generated contrastive dissimilarity space\n",
    "  with open(contr_diss_space_filename, \"wb\") as f:\n",
    "    pickle.dump(contr_diss_space, f, -1)\n",
    "\n",
    "# Classify using the contrastive dissimilarity space\n",
    "np.random.seed(seed)\n",
    "\n",
    "svm_parameters = [\n",
    "  {'kernel': ['rbf'],\n",
    "   'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "   'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}]\n",
    "\n",
    "# Loop through different train datasets\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Initialize best F1 score\n",
    "  best_f1 = 0\n",
    "\n",
    "  # List of classifiers to be used\n",
    "  classifiers = [\n",
    "    sklearn.model_selection.GridSearchCV(sklearn.svm.SVC(), svm_parameters, cv = 5, scoring = 'f1_macro', verbose = 0),\n",
    "    sklearn.ensemble.RandomForestClassifier(),\n",
    "    sklearn.naive_bayes.GaussianNB()\n",
    "  ]\n",
    "\n",
    "  for clf in classifiers:\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(contr_diss_space[i][\"train\"], Y_train[i])\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = clf.predict(contr_diss_space[i][\"test\"])\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "\n",
    "    # Update best F1 score\n",
    "    best_f1 = max(best_f1, f1)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"Contr. space\", f\"Fold_{i}\"] = best_f1\n",
    "\n",
    "  print(f\"{i} -> {best_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dissimilarity vector\n",
    "\n",
    "# Initialize variables\n",
    "# In this case, we do not store the precomputed features into the disk due to its large size\n",
    "diss_vector = {}\n",
    "\n",
    "# Loop through each fold\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Initialize dictionary to store the dissimilarity vectors\n",
    "  diss_vector[i] = {}\n",
    "\n",
    "  # Generate a list of unique classes and replicate it for each prototype\n",
    "  prot_Y = np.repeat(np.unique(Y_train[i]), n_prototypes)\n",
    "\n",
    "  # Create dissimilarity vectors for training labels\n",
    "  # A pair is similar (True) if both labels are the same, otherwise dissimilar (False)\n",
    "  diss_vector[i][\"Y_train\"] = np.transpose([np.repeat(Y_train[i], len(prot_Y)), np.tile(prot_Y, len(Y_train[i]))])\n",
    "  diss_vector[i][\"Y_train\"] = diss_vector[i][\"Y_train\"][:,0] == diss_vector[i][\"Y_train\"][:,1]\n",
    "\n",
    "  # Create dissimilarity vectors for test labels\n",
    "  # A pair is similar (True) if both labels are the same, otherwise dissimilar (False)\n",
    "  diss_vector[i][\"Y_test\"] = np.transpose([np.repeat(Y_test, len(prot_Y)), np.tile(prot_Y, len(Y_test))])\n",
    "  diss_vector[i][\"Y_test\"] = diss_vector[i][\"Y_test\"][:,0] == diss_vector[i][\"Y_test\"][:,1]\n",
    "\n",
    "  # Extract dimensions for reshaping\n",
    "  n_features = X_train[i].shape[1]\n",
    "\n",
    "  # Expand dimensions to facilitate element-wise subtraction with prototypes\n",
    "  enc_train = X_train[i][:,np.newaxis,:]\n",
    "  enc_test = X_test[i][:,np.newaxis,:]\n",
    "\n",
    "  # Make prototypes compatible for element-wise operations\n",
    "  local_kmeans = prototypes[i][np.newaxis, :, :]\n",
    "\n",
    "  # Calculate absolute difference between each feature and prototype for training and test sets\n",
    "  diss_vector[i][\"X_train\"] = np.abs(enc_train - local_kmeans).reshape(-1, n_features)\n",
    "  diss_vector[i][\"X_test\"] = np.abs(enc_test - local_kmeans).reshape(-1, n_features)\n",
    "\n",
    "\n",
    "# Classify using the dissimilarity vectors\n",
    "np.random.seed(seed)\n",
    "\n",
    "svm_parameters = [\n",
    "  {'kernel': ['rbf'],\n",
    "   'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "   'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}]\n",
    "\n",
    "avg_prototypes = True\n",
    "\n",
    "# Loop through different train datasets\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Initialize best F1 score\n",
    "  best_f1 = 0\n",
    "\n",
    "  # List of classifiers to be used\n",
    "  classifiers = [\n",
    "    sklearn.model_selection.GridSearchCV(sklearn.svm.SVC(probability = True), svm_parameters, cv = 5, scoring = 'f1_macro', verbose = 0),\n",
    "    sklearn.ensemble.RandomForestClassifier(),\n",
    "    sklearn.naive_bayes.GaussianNB()\n",
    "  ]\n",
    "\n",
    "  for clf in classifiers:\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(diss_vector[i][\"X_train\"], diss_vector[i][\"Y_train\"])\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = clf.predict(diss_vector[i][\"X_test\"])\n",
    "\n",
    "    # Reshape to match the number of test samples\n",
    "    Y_pred = np.reshape(Y_pred[:, 1], (Y_test.shape[0], -1))\n",
    "\n",
    "    # Avg. the prediction for all prototypes\n",
    "    if avg_prototypes:\n",
    "      Y_pred = np.reshape(Y_pred, (Y_test.shape[0], -1, n_prototypes))\n",
    "      Y_pred = np.mean(Y_pred, axis = -1)\n",
    "      Y_pred = np.argmax(Y_pred, axis = 1)\n",
    "    # Get the maximum probability as the prediction\n",
    "    else:\n",
    "      Y_pred = np.argmax(Y_pred, axis = 1) // n_prototypes\n",
    "\n",
    "    f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"Vector\", f\"Fold_{i}\"] = best_f1\n",
    "\n",
    "  print(f\"{i} -> {best_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive dissimilarity vector\n",
    "\n",
    "# Initialize variables\n",
    "contr_diss_vector_filename = \"cache/contr_dissimilarity_vector.pkl\"\n",
    "\n",
    "# Load precomputed contrastive dissimilarity space if exists\n",
    "if os.path.isfile(contr_diss_vector_filename):\n",
    "  with open(contr_diss_vector_filename, \"rb\") as f:\n",
    "    contr_diss_vector = pickle.load(f)\n",
    "\n",
    "else:\n",
    "\n",
    "  # Initialize dictionary to store the contr. dissimilarity vectors\n",
    "  contr_diss_vector = {}\n",
    "\n",
    "  # Loop through each fold\n",
    "  for i in range(1, 11):\n",
    "    contr_diss_vector[i] = {\"X_train\": [], \"X_test\": []}\n",
    "\n",
    "    # Generate a list of unique classes and replicate it for each prototype\n",
    "    prot_Y = np.repeat(np.unique(Y_train[i]), n_prototypes)\n",
    "\n",
    "    # Generate label pairs for training and testing data\n",
    "    contr_diss_vector[i][\"Y_train\"] = np.transpose([np.repeat(Y_train[i], len(prot_Y)), np.tile(prot_Y, len(Y_train[i]))])\n",
    "    contr_diss_vector[i][\"Y_train\"] = contr_diss_vector[i][\"Y_train\"][:,0] == contr_diss_vector[i][\"Y_train\"][:,1]\n",
    "\n",
    "    contr_diss_vector[i][\"Y_test\"] = np.transpose([np.repeat(Y_test, len(prot_Y)), np.tile(prot_Y, len(Y_test))])\n",
    "    contr_diss_vector[i][\"Y_test\"] = contr_diss_vector[i][\"Y_test\"][:,0] == contr_diss_vector[i][\"Y_test\"][:,1]\n",
    "\n",
    "    # Enable training mode to activate dropout\n",
    "    # This way the same input generates slightly different outputs that we treat as \"augmentations\"\n",
    "    models[i].train()\n",
    "\n",
    "    # Number of patches per prototype\n",
    "    number_patches = 50\n",
    "\n",
    "    # Number of prototypes\n",
    "    number_prototypes = prototypes[i].shape[0]\n",
    "\n",
    "    # Calculate dissimilarity vectors for both training and testing data\n",
    "    for key, dataset in {\"X_train\": X_train[i], \"X_test\": X_test[i]}.items():\n",
    "\n",
    "      # Loop through each data point in the dataset\n",
    "      for idx in range(dataset.shape[0]):\n",
    "\n",
    "        # Prepare patches and prototypes for the projection head\n",
    "        local_patches = torch.from_numpy(np.float32(dataset[idx])).to(\"cuda\")\n",
    "        local_patches = torch.tile(local_patches, [number_prototypes * number_patches, 1])\n",
    "\n",
    "        local_prototypes = np.repeat(prototypes[i], number_patches, axis = 0)\n",
    "        local_prototypes = torch.from_numpy(local_prototypes).to(\"cuda\")\n",
    "\n",
    "        # Calculate the dissimilarity vector using the model's projection head\n",
    "        diss_vector = models[i].projection_head(local_patches, local_prototypes)\n",
    "        diss_vector = np.array(np.split(diss_vector.cpu().numpy(), number_prototypes))\n",
    "\n",
    "        contr_diss_vector[i][key].append(diss_vector)\n",
    "\n",
    "    # Reshape to match the labels\n",
    "    contr_diss_vector[i][\"X_train\"] = np.reshape(contr_diss_vector[i][\"X_train\"], (len(Y_train[i]) * number_prototypes, number_patches))\n",
    "    contr_diss_vector[i][\"X_test\"] = np.reshape(contr_diss_vector[i][\"X_test\"], (len(Y_test) * number_prototypes, number_patches))\n",
    "\n",
    "    # Switch back to evaluation mode\n",
    "    models[i].eval()\n",
    "\n",
    "  # Save the generated contrastive dissimilarity vectors\n",
    "  with open(contr_diss_vector_filename, \"wb\") as f:\n",
    "    pickle.dump(contr_diss_vector, f, -1)\n",
    "\n",
    "# Classify using the contrastive dissimilarity vectors\n",
    "np.random.seed(seed)\n",
    "\n",
    "svm_parameters = [\n",
    "  {'kernel': ['rbf'],\n",
    "   'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "   'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}]\n",
    "\n",
    "avg_prototypes = True\n",
    "\n",
    "# Loop through different train datasets\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Initialize best F1 score\n",
    "  best_f1 = 0\n",
    "\n",
    "  # List of classifiers to be used\n",
    "  classifiers = [\n",
    "    sklearn.model_selection.GridSearchCV(sklearn.svm.SVC(probability = True), svm_parameters, cv = 3, scoring = 'f1_macro', verbose = 0),\n",
    "    sklearn.ensemble.RandomForestClassifier(),\n",
    "    sklearn.naive_bayes.GaussianNB()\n",
    "  ]\n",
    "\n",
    "  for clf in classifiers:\n",
    "\n",
    "    # Fit the model\n",
    "    clf.fit(contr_diss_vector[i][\"X_train\"], contr_diss_vector[i][\"Y_train\"])\n",
    "\n",
    "    # Make predictions\n",
    "    Y_pred = clf.predict_proba(contr_diss_vector[i][\"X_test\"])\n",
    "\n",
    "    # Reshape to match the number of test samples\n",
    "    Y_pred = np.reshape(Y_pred[:, 1], (Y_test.shape[0], -1))\n",
    "\n",
    "    # Avg. the prediction for all prototypes\n",
    "    if avg_prototypes:\n",
    "      Y_pred = np.reshape(Y_pred, (Y_test.shape[0], -1, n_prototypes))\n",
    "      Y_pred = np.mean(Y_pred, axis = -1)\n",
    "      Y_pred = np.argmax(Y_pred, axis = 1)\n",
    "    # Get the maximum probability as the prediction\n",
    "    else:\n",
    "      Y_pred = np.argmax(Y_pred, axis = 1) // n_prototypes\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "\n",
    "    # Update best F1 score\n",
    "    best_f1 = max(best_f1, f1)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"Contr. vector\", f\"Fold_{i}\"] = best_f1\n",
    "\n",
    "  print(f\"{i} -> {best_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Classification\n",
    "np.random.seed(seed)\n",
    "\n",
    "svm_parameters = [\n",
    "  {'kernel': ['rbf'],\n",
    "   'gamma': [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "   'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]}]\n",
    "\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Original dataset\n",
    "  # Initialize SVM classifier\n",
    "  svc = sklearn.svm.SVC()\n",
    "\n",
    "  # Create the grid search with cross-validation\n",
    "  clf = sklearn.model_selection.GridSearchCV(svc, svm_parameters, cv = 5, scoring = 'f1_macro', verbose = 0)\n",
    "\n",
    "  # Fit the model\n",
    "  clf.fit(X_train[i], Y_train[i])\n",
    "\n",
    "  # Get the best parameters and estimator\n",
    "  best_params = clf.best_params_\n",
    "  best_estimator = clf.best_estimator_\n",
    "\n",
    "  # Make predictions using the best estimator\n",
    "  Y_pred = best_estimator.predict(X_test[i])\n",
    "\n",
    "  f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "  acc = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"SVM\", f\"Fold_{i}\"] = f1\n",
    "\n",
    "  print(f\"{i} -> {f1:.2f}\")\n",
    "\n",
    "  # Augmented dataset\n",
    "  # Initialize SVM classifier\n",
    "  svc = sklearn.svm.SVC()\n",
    "\n",
    "  # Create the grid search with cross-validation\n",
    "  clf = sklearn.model_selection.GridSearchCV(svc, svm_parameters, cv = 5, scoring = 'f1_macro', verbose = 0)\n",
    "\n",
    "  # Fit the model\n",
    "  clf.fit(X_train_smote[i], Y_train_smote[i])\n",
    "\n",
    "  # Get the best parameters and estimator\n",
    "  best_params = clf.best_params_\n",
    "  best_estimator = clf.best_estimator_\n",
    "\n",
    "  # Make predictions using the best estimator\n",
    "  Y_pred = best_estimator.predict(X_test[i])\n",
    "\n",
    "  f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "  acc = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"SVM (smote)\", f\"Fold_{i}\"] = f1\n",
    "\n",
    "  print(f\"{i} -> {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "np.random.seed(1234)\n",
    "\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Original dataset\n",
    "  # Initialize Random Forest classifier\n",
    "  clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "  # Fit the model\n",
    "  clf.fit(X_train[i], Y_train[i])\n",
    "\n",
    "  # Make predictions using the best estimator\n",
    "  Y_pred = clf.predict(X_test[i])\n",
    "\n",
    "  f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "  acc = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"RF\", f\"Fold_{i}\"] = f1\n",
    "\n",
    "  print(f\"{i} -> {f1:.2f}\")\n",
    "\n",
    "  # Augmented dataset\n",
    "  # Initialize Random Forest classifier\n",
    "  clf = sklearn.ensemble.RandomForestClassifier()\n",
    "\n",
    "  # Fit the model\n",
    "  clf.fit(X_train_smote[i], Y_train_smote[i])\n",
    "\n",
    "  # Make predictions using the best estimator\n",
    "  Y_pred = clf.predict(X_test[i])\n",
    "\n",
    "  f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "  acc = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"RF (smote)\", f\"Fold_{i}\"] = f1\n",
    "\n",
    "  print(f\"{i} -> {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classification\n",
    "np.random.seed(1234)\n",
    "\n",
    "for i in range(1, 11):\n",
    "\n",
    "  # Original dataset\n",
    "  # Initialize Random Forest classifier\n",
    "  clf = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "  # Fit the model\n",
    "  clf.fit(X_train[i], Y_train[i])\n",
    "\n",
    "  # Make predictions using the best estimator\n",
    "  Y_pred = clf.predict(X_test[i])\n",
    "\n",
    "  f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "  acc = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"NB\", f\"Fold_{i}\"] = f1\n",
    "\n",
    "  print(f\"{i} -> {f1:.2f}\")\n",
    "\n",
    "  # Augmented dataset\n",
    "  # Initialize Random Forest classifier\n",
    "  clf = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "  # Fit the model\n",
    "  clf.fit(X_train_smote[i], Y_train_smote[i])\n",
    "\n",
    "  # Make predictions using the best estimator\n",
    "  Y_pred = clf.predict(X_test[i])\n",
    "\n",
    "  f1 = sklearn.metrics.f1_score(Y_test, Y_pred, average = \"macro\")\n",
    "  acc = sklearn.metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "  # Add to the f1 data frame\n",
    "  f1_scores.loc[\"NB (smote)\", f\"Fold_{i}\"] = f1\n",
    "\n",
    "  print(f\"{i} -> {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best classifier for each fold\n",
    "f1_scores = f1_scores.astype(float).round(2)\n",
    "for i in range(1, 11):\n",
    "  fold = f\"Fold_{i}\"\n",
    "  best_f1 = f1_scores[fold].max()\n",
    "  max_rows = f1_scores[f1_scores[fold] == best_f1].index.tolist()\n",
    "  print(f\"For {fold}, the max F1 score is {best_f1:.2f}, achieved by: {max_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_names = [\"SVM\", \"RF\", \"NB\"]\n",
    "f1_scores.loc[\"Traditional\"] = f1_scores.loc[row_names].max()\n",
    "\n",
    "row_names = [\"SVM (smote)\", \"RF (smote)\", \"NB (smote)\"]\n",
    "f1_scores.loc[\"Traditional (smote)\"] = f1_scores.loc[row_names].max()\n",
    "\n",
    "f1_tbl = f1_scores.apply(lambda row: row.name + ' & ' + ' & '.join(row.astype(str)) + r' \\\\ \\hline', axis = 1).to_list()\n",
    "for row in f1_tbl:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute silhouette score\n",
    "sil_scores = []\n",
    "for i in range(1, 11):\n",
    "  sil_score = sklearn.metrics.silhouette_score(contr_diss_space[i][\"train\"], Y_train[i])\n",
    "  sil_scores.append(sil_score)\n",
    "\n",
    "# Calculate average and standard deviation\n",
    "avg_sil = round(np.mean(sil_scores), 2)\n",
    "sd_sil = round(np.std(sil_scores), 2)\n",
    "\n",
    "print(f\"{avg_sil:.2f} ({sd_sil:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
